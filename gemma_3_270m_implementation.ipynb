{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP85poN8EyiTLMlftcvl5gB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/p1kalys/Gemma-3-270M/blob/main/gemma_3_270m_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "4odORH0nS3xf"
      },
      "outputs": [],
      "source": [
        "!pip install -r https://raw.githubusercontent.com/p1kalys/Gemma-3-270M/refs/heads/main/requirements.txt --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "pkgs = [\n",
        "    \"huggingface_hub\",  # to download pretrained weights\n",
        "    \"tokenizers\",       # to implement the tokenizer\n",
        "    \"torch\",            # to implement the model\n",
        "]\n",
        "for p in pkgs:\n",
        "    print(f\"{p} version: {version(p)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4eoVnO-UBhH",
        "outputId": "532809e0-b128-44c1-9aa0-00d9a41c1664"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "huggingface_hub version: 0.34.4\n",
            "tokenizers version: 0.21.4\n",
            "torch version: 2.8.0+cu126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "USE_INSTRUCT_MODEL = True"
      ],
      "metadata": {
        "id": "i1UEc1iYUNJc"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Architecture**"
      ],
      "metadata": {
        "id": "CDT-CRntUZAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Applies non linear transformation to the input tensor using two parallel linear layers, GELU (Guassian Error Linear Unit) activation function and a final linear layer to project output back to the original embedding dimension\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
        "    self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
        "    self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_fc1 = self.fc1(x)\n",
        "    x_fc2 = self.fc2(x)\n",
        "    x = nn.functional.gelu(x_fc1, approximate=\"tanh\") * x_fc2\n",
        "    return self.fc3(x)"
      ],
      "metadata": {
        "id": "KlazpD65URaU"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizes the output of layers in the Gemma model\n",
        "class RMSNorm(nn.Module):\n",
        "  def __init__(self, emb_dim, eps=1e-6, bias=False):\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    # Gemma3 stores zero-centered weights and uses (1 + weight) during forward\n",
        "    self.scale = nn.Parameter(torch.zeros(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim)) if bias else None\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Match HF Gemma3: compute norm in float32, then scale by (1 + w)\n",
        "    input_dtype = x.dtype\n",
        "    x_f = x.float()\n",
        "    var = x_f.pow(2).mean(dim=-1, keepdim=True)\n",
        "    x_norm = x_f * torch.rsqrt(var + self.eps)\n",
        "    out = x_norm * (1.0 + self.scale.float())\n",
        "\n",
        "    if self.shift is not None:\n",
        "      out = out + self.shift.float()\n",
        "\n",
        "    return out.to(input_dtype)"
      ],
      "metadata": {
        "id": "lxo6hUMZUT0x"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Rotary Position Embeddings (RoPE) - injects positional information into the attention mechanism of a transformer model\n",
        "def compute_rope_params(head_dim, theta_base=10_000, context_length=4096, dtype=torch.float32):\n",
        "  assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
        "\n",
        "  # Compute the inverse frequencies\n",
        "  inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype=dtype)[: (head_dim // 2)].float() / head_dim))\n",
        "\n",
        "  # Generate position indices\n",
        "  positions = torch.arange(context_length, dtype=dtype)\n",
        "\n",
        "  # Compute the angles\n",
        "  angles = positions[:, None] * inv_freq[None, :]  # Shape: (context_length, head_dim // 2)\n",
        "\n",
        "  # Expand angles to match the head_dim\n",
        "  angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n",
        "\n",
        "  # Precompute sine and cosine\n",
        "  cos = torch.cos(angles)\n",
        "  sin = torch.sin(angles)\n",
        "\n",
        "  return cos, sin"
      ],
      "metadata": {
        "id": "-nFKsC4OVds6"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying RoPE to input tensor `x`\n",
        "def apply_rope(x, cos, sin):\n",
        "  # x: (batch_size, num_heads, seq_len, head_dim)\n",
        "  batch_size, num_heads, seq_len, head_dim = x.shape\n",
        "  assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
        "\n",
        "  # Split x into first half and second half\n",
        "  x1 = x[..., : head_dim // 2]  # First half\n",
        "  x2 = x[..., head_dim // 2 :]  # Second half\n",
        "\n",
        "  # Adjust sin and cos shapes\n",
        "  cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim)\n",
        "  sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "  # Apply the rotary transformation\n",
        "  rotated = torch.cat((-x2, x1), dim=-1)\n",
        "  x_rotated = (x * cos) + (rotated * sin)\n",
        "\n",
        "  # It's ok to use lower-precision after applying cos and sin rotation\n",
        "  return x_rotated.to(dtype=x.dtype)"
      ],
      "metadata": {
        "id": "d7_dDmstVnYp"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GQA - an optimization of standard multi-head attention which is the grouping of key and value heads to reduce computation.\n",
        "class GroupedQueryAttention(nn.Module):\n",
        "  def __init__(self, d_in, num_heads, num_kv_groups, head_dim=None, qk_norm=False, query_pre_attn_scalar=None, dtype=None):\n",
        "    super().__init__()\n",
        "    assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
        "\n",
        "    self.num_heads = num_heads\n",
        "    self.num_kv_groups = num_kv_groups\n",
        "    self.group_size = num_heads // num_kv_groups\n",
        "\n",
        "    if head_dim is None:\n",
        "      assert d_in % num_heads == 0, \"`d_in` must be divisible by `num_heads` if `head_dim` is not set\"\n",
        "      head_dim = d_in // num_heads\n",
        "\n",
        "    self.head_dim = head_dim\n",
        "    self.d_out = num_heads * head_dim\n",
        "\n",
        "    self.W_query = nn.Linear(d_in, self.d_out, bias=False, dtype=dtype)\n",
        "    self.W_key = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
        "    self.W_value = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
        "\n",
        "    self.out_proj = nn.Linear(self.d_out, d_in, bias=False, dtype=dtype)\n",
        "\n",
        "    if qk_norm:\n",
        "      self.q_norm = RMSNorm(head_dim, eps=1e-6)\n",
        "      self.k_norm = RMSNorm(head_dim, eps=1e-6)\n",
        "    else:\n",
        "      self.q_norm = self.k_norm = None\n",
        "\n",
        "    if query_pre_attn_scalar is not None:\n",
        "      self.scaling = (query_pre_attn_scalar) ** -0.5\n",
        "    else:\n",
        "      self.scaling = (head_dim) ** -0.5\n",
        "\n",
        "\n",
        "  def forward(self, x, mask, cos, sin):\n",
        "    b, num_tokens, _ = x.shape\n",
        "\n",
        "    # Apply projections\n",
        "    queries = self.W_query(x)  # (b, num_tokens, num_heads * head_dim)\n",
        "    keys = self.W_key(x)       # (b, num_tokens, num_kv_groups * head_dim)\n",
        "    values = self.W_value(x)   # (b, num_tokens, num_kv_groups * head_dim)\n",
        "\n",
        "    # Reshape\n",
        "    queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "    keys = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
        "    values = values.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
        "\n",
        "    # Optional normalization\n",
        "    if self.q_norm:\n",
        "      queries = self.q_norm(queries)\n",
        "    if self.k_norm:\n",
        "      keys = self.k_norm(keys)\n",
        "\n",
        "    # Apply RoPE\n",
        "    queries = apply_rope(queries, cos, sin)\n",
        "    keys = apply_rope(keys, cos, sin)\n",
        "\n",
        "    # Expand K and V to match number of heads\n",
        "    keys = keys.repeat_interleave(self.group_size, dim=1)\n",
        "    values = values.repeat_interleave(self.group_size, dim=1)\n",
        "\n",
        "    # Scale queries\n",
        "    queries = queries * self.scaling\n",
        "\n",
        "    # Attention\n",
        "    attn_scores = queries @ keys.transpose(2, 3)\n",
        "    attn_scores = attn_scores.masked_fill(mask, -torch.inf)\n",
        "    attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "    context = (attn_weights @ values).transpose(1, 2).reshape(b, num_tokens, self.d_out)\n",
        "    return self.out_proj(context)"
      ],
      "metadata": {
        "id": "H5PPq--PWdVo"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TransformerBlock processes the input through a sequence of normalization, attention, residual connection, normalization, feed-forward, and another residual connection.\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg: dict, attn_type: str):\n",
        "    super().__init__()\n",
        "    self.attn_type = attn_type\n",
        "    self.att = GroupedQueryAttention(\n",
        "        d_in=cfg[\"emb_dim\"],\n",
        "        num_heads=cfg[\"n_heads\"],\n",
        "        num_kv_groups=cfg[\"n_kv_groups\"],\n",
        "        head_dim=cfg[\"head_dim\"],\n",
        "        qk_norm=cfg[\"qk_norm\"],\n",
        "        query_pre_attn_scalar=cfg[\"query_pre_attn_scalar\"],\n",
        "        dtype=cfg[\"dtype\"])\n",
        "    self.ff = FeedForward(cfg)\n",
        "    self.input_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
        "    self.post_attention_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
        "    self.pre_feedforward_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
        "    self.post_feedforward_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
        "\n",
        "  def forward(self, x, mask_global, mask_local, cos_global, sin_global, cos_local, sin_local):\n",
        "    # Shortcut connection for attention block\n",
        "    shortcut = x\n",
        "    x = self.input_layernorm(x)\n",
        "\n",
        "    if self.attn_type == \"sliding_attention\":\n",
        "      attn_mask = mask_local\n",
        "      cos = cos_local\n",
        "      sin = sin_local\n",
        "    else:\n",
        "      attn_mask = mask_global\n",
        "      cos = cos_global\n",
        "      sin = sin_global\n",
        "\n",
        "    x_attn = self.att(x, attn_mask, cos, sin)\n",
        "    x_attn = self.post_attention_layernorm(x_attn)\n",
        "    x = shortcut + x_attn\n",
        "\n",
        "    # Shortcut connection for feed forward block\n",
        "    shortcut = x\n",
        "    x_ffn = self.pre_feedforward_layernorm(x)\n",
        "    x_ffn = self.ff(x_ffn)\n",
        "    x_ffn = self.post_feedforward_layernorm(x_ffn)\n",
        "    x = shortcut + x_ffn\n",
        "    return x"
      ],
      "metadata": {
        "id": "QXEwkKZTW-dK"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Orchestrates the flow of data through the entire network, applying token embeddings, multiple transformer blocks with different attention mechanisms and masking, final normalization, and an output head to produce the final token logits.\n",
        "class Gemma3Model(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    assert cfg[\"layer_types\"] is not None and len(cfg[\"layer_types\"]) == cfg[\"n_layers\"]\n",
        "\n",
        "    # Main model parameters\n",
        "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
        "\n",
        "    self.blocks = nn.ModuleList([\n",
        "        TransformerBlock(cfg, attn_type)for attn_type in cfg[\"layer_types\"]\n",
        "    ])\n",
        "\n",
        "    self.final_norm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
        "    self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
        "    self.cfg = cfg\n",
        "\n",
        "    # Reusuable utilities\n",
        "    cos_local, sin_local = compute_rope_params(\n",
        "        head_dim=cfg[\"head_dim\"],\n",
        "        theta_base=cfg[\"rope_local_base\"],\n",
        "        context_length=cfg[\"context_length\"],\n",
        "        dtype=torch.float32,\n",
        "    )\n",
        "    cos_global, sin_global = compute_rope_params(\n",
        "        head_dim=cfg[\"head_dim\"],\n",
        "        theta_base=cfg[\"rope_base\"],\n",
        "        context_length=cfg[\"context_length\"],\n",
        "        dtype=torch.float32,\n",
        "    )\n",
        "    self.register_buffer(\"cos_local\", cos_local, persistent=False)\n",
        "    self.register_buffer(\"sin_local\", sin_local, persistent=False)\n",
        "    self.register_buffer(\"cos_global\", cos_global, persistent=False)\n",
        "    self.register_buffer(\"sin_global\", sin_global, persistent=False)\n",
        "\n",
        "  def _create_masks(self, seq_len, device):\n",
        "    ones = torch.ones((seq_len, seq_len), dtype=torch.bool, device=device)\n",
        "\n",
        "    # mask_global (future is masked: j > i)\n",
        "    #     j:  0 1 2 3 4 5 6 7\n",
        "    #  i\n",
        "    #     0:  0 1 1 1 1 1 1 1\n",
        "    #     1:  0 0 1 1 1 1 1 1\n",
        "    #     2:  0 0 0 1 1 1 1 1\n",
        "    #     3:  0 0 0 0 1 1 1 1\n",
        "    #     4:  0 0 0 0 0 1 1 1\n",
        "    #     5:  0 0 0 0 0 0 1 1\n",
        "    #     6:  0 0 0 0 0 0 0 1\n",
        "    #     7:  0 0 0 0 0 0 0 0\n",
        "    mask_global = torch.triu(ones, diagonal=1)\n",
        "\n",
        "    # far_past (too far back is masked: i - j >= sliding_window)\n",
        "    # where sliding_window = 4\n",
        "    #     j:  0 1 2 3 4 5 6 7\n",
        "    #  i\n",
        "    #     0:  0 0 0 0 0 0 0 0\n",
        "    #     1:  0 0 0 0 0 0 0 0\n",
        "    #     2:  0 0 0 0 0 0 0 0\n",
        "    #     3:  0 0 0 0 0 0 0 0\n",
        "    #     4:  1 0 0 0 0 0 0 0\n",
        "    #     5:  1 1 0 0 0 0 0 0\n",
        "    #     6:  1 1 1 0 0 0 0 0\n",
        "    #     7:  1 1 1 1 0 0 0 0\n",
        "    far_past = torch.triu(ones, diagonal=self.cfg[\"sliding_window\"]).T\n",
        "\n",
        "    # Local (sliding_window) = future OR far-past\n",
        "    # mask_local\n",
        "    #     j:  0 1 2 3 4 5 6 7\n",
        "    # i\n",
        "    # 0:      0 1 1 1 1 1 1 1\n",
        "    # 1:      0 0 1 1 1 1 1 1\n",
        "    # 2:      0 0 0 1 1 1 1 1\n",
        "    # 3:      0 0 0 0 1 1 1 1\n",
        "    # 4:      1 0 0 0 0 1 1 1\n",
        "    # 5:      1 1 0 0 0 0 1 1\n",
        "    # 6:      1 1 1 0 0 0 0 1\n",
        "    # 7:      1 1 1 1 0 0 0 0\n",
        "    mask_local = mask_global | far_past\n",
        "    return mask_global, mask_local\n",
        "\n",
        "  def forward(self, input_ids):\n",
        "    # Forward pass\n",
        "    b, seq_len = input_ids.shape\n",
        "    x = self.tok_emb(input_ids) * (self.cfg[\"emb_dim\"] ** 0.5)\n",
        "    mask_global, mask_local = self._create_masks(seq_len, x.device)\n",
        "\n",
        "    for block in self.blocks:\n",
        "      x = block(\n",
        "          x,\n",
        "          mask_global=mask_global,\n",
        "          mask_local=mask_local,\n",
        "          cos_global=self.cos_global,\n",
        "          sin_global=self.sin_global,\n",
        "          cos_local=self.cos_local,\n",
        "          sin_local=self.sin_local,\n",
        "      )\n",
        "\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x.to(self.cfg[\"dtype\"]))\n",
        "    return logits"
      ],
      "metadata": {
        "id": "BxW9Stc2XW9W"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Initialize model**"
      ],
      "metadata": {
        "id": "Y9xZqsSsXtSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GEMMA3_CONFIG_270M = {\n",
        "    \"vocab_size\": 262_144,\n",
        "    \"context_length\": 32_768,\n",
        "    \"emb_dim\": 640,\n",
        "    \"n_heads\": 4,\n",
        "    \"n_layers\": 18,\n",
        "    \"hidden_dim\": 2048,\n",
        "    \"head_dim\": 256,\n",
        "    \"qk_norm\": True,\n",
        "    \"n_kv_groups\": 1,\n",
        "    \"rope_local_base\": 10_000.0,\n",
        "    \"rope_base\": 1_000_000.0,\n",
        "    \"sliding_window\": 512,\n",
        "      \"layer_types\": [\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"full_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"full_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"full_attention\"\n",
        "    ],\n",
        "    \"dtype\": torch.bfloat16,\n",
        "    \"query_pre_attn_scalar\": 256,\n",
        "}"
      ],
      "metadata": {
        "id": "_GOalGWBXqDB"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = Gemma3Model(GEMMA3_CONFIG_270M)"
      ],
      "metadata": {
        "id": "8QwN950CXx-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "XN0SLEWGX0Ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(torch.tensor([1, 2, 3]).unsqueeze(0))"
      ],
      "metadata": {
        "id": "gH6od1u9Ysy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params:,}\")\n",
        "\n",
        "# Account for weight tying\n",
        "total_params_normalized = total_params - model.tok_emb.weight.numel()\n",
        "print(f\"\\nTotal number of unique parameters: {total_params_normalized:,}\")"
      ],
      "metadata": {
        "id": "swD9yD2FYwHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_memory_size(model, input_dtype=torch.float32):\n",
        "  total_params = 0\n",
        "  total_grads = 0\n",
        "  for param in model.parameters():\n",
        "    # Calculate total number of elements per parameter\n",
        "    param_size = param.numel()\n",
        "    total_params += param_size\n",
        "    # Check if gradients are stored for this parameter\n",
        "    if param.requires_grad:\n",
        "      total_grads += param_size\n",
        "\n",
        "  # Calculate buffer size (non-parameters that require memory)\n",
        "  total_buffers = sum(buf.numel() for buf in model.buffers())\n",
        "\n",
        "  # Size in bytes = (Number of elements) * (Size of each element in bytes)\n",
        "  # We assume parameters and gradients are stored in the same type as input dtype\n",
        "  element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
        "  total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
        "\n",
        "  # Convert bytes to gigabytes\n",
        "  total_memory_gb = total_memory_bytes / (1024**3)\n",
        "\n",
        "  return total_memory_gb\n",
        "\n",
        "print(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.2f} GB\")\n",
        "print(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")"
      ],
      "metadata": {
        "id": "CE0I6eOWYysP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "model.to(device);"
      ],
      "metadata": {
        "id": "uXz0OJIBY7At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Load pretrained weights**"
      ],
      "metadata": {
        "id": "uZTN3iXJZIQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transfers the learned knowledge from a pretrained model to custom model implementation, allowing us to leverage the benefits of pretraining without training from scratch.\n",
        "def load_weights_into_gemma(Gemma3Model, param_config, params):\n",
        "  def assign(left, right, tensor_name=\"unknown\"):\n",
        "    if left.shape != right.shape:\n",
        "      raise ValueError(\n",
        "          f\"Shape mismatch in tensor '{tensor_name}'. Left: {left.shape}, Right: {right.shape}\"\n",
        "      )\n",
        "    return torch.nn.Parameter(right.clone().detach() if isinstance(right, torch.Tensor) else torch.tensor(right))\n",
        "\n",
        "  # Embedding weights\n",
        "  if \"model.embed_tokens.weight\" in params:\n",
        "    model.tok_emb.weight = assign(\n",
        "        model.tok_emb.weight,\n",
        "        params[\"model.embed_tokens.weight\"],\n",
        "        \"model.embed_tokens.weight\",\n",
        "    )\n",
        "\n",
        "  # Iterate over transformer layers\n",
        "  for l in range(param_config[\"n_layers\"]):\n",
        "    block = model.blocks[l]\n",
        "    att = block.att\n",
        "    # Attention projections\n",
        "    att.W_query.weight = assign(\n",
        "        att.W_query.weight,\n",
        "        params[f\"model.layers.{l}.self_attn.q_proj.weight\"],\n",
        "        f\"model.layers.{l}.self_attn.q_proj.weight\",\n",
        "    )\n",
        "    att.W_key.weight = assign(\n",
        "        att.W_key.weight,\n",
        "        params[f\"model.layers.{l}.self_attn.k_proj.weight\"],\n",
        "        f\"model.layers.{l}.self_attn.k_proj.weight\",\n",
        "    )\n",
        "    att.W_value.weight = assign(\n",
        "        att.W_value.weight,\n",
        "        params[f\"model.layers.{l}.self_attn.v_proj.weight\"],\n",
        "        f\"model.layers.{l}.self_attn.v_proj.weight\",\n",
        "    )\n",
        "    att.out_proj.weight = assign(\n",
        "        att.out_proj.weight,\n",
        "        params[f\"model.layers.{l}.self_attn.o_proj.weight\"],\n",
        "        f\"model.layers.{l}.self_attn.o_proj.weight\",\n",
        "    )\n",
        "    # QK normalization weights\n",
        "    att.q_norm.scale = assign(\n",
        "        att.q_norm.scale,\n",
        "        params[f\"model.layers.{l}.self_attn.q_norm.weight\"],\n",
        "        f\"model.layers.{l}.self_attn.q_norm.weight\",\n",
        "    )\n",
        "    att.k_norm.scale = assign(\n",
        "        att.k_norm.scale,\n",
        "        params[f\"model.layers.{l}.self_attn.k_norm.weight\"],\n",
        "        f\"model.layers.{l}.self_attn.k_norm.weight\",\n",
        "    )\n",
        "    # Feed forward weights\n",
        "    block.ff.fc1.weight = assign(\n",
        "        block.ff.fc1.weight,\n",
        "        params[f\"model.layers.{l}.mlp.gate_proj.weight\"],\n",
        "        f\"model.layers.{l}.mlp.gate_proj.weight\",\n",
        "    )\n",
        "    block.ff.fc2.weight = assign(\n",
        "        block.ff.fc2.weight,\n",
        "        params[f\"model.layers.{l}.mlp.up_proj.weight\"],\n",
        "        f\"model.layers.{l}.mlp.up_proj.weight\",\n",
        "    )\n",
        "    block.ff.fc3.weight = assign(\n",
        "        block.ff.fc3.weight,\n",
        "        params[f\"model.layers.{l}.mlp.down_proj.weight\"],\n",
        "        f\"model.layers.{l}.mlp.down_proj.weight\",\n",
        "    )\n",
        "    # LayerNorm weights\n",
        "    block.input_layernorm.scale = assign(\n",
        "        block.input_layernorm.scale,\n",
        "        params[f\"model.layers.{l}.input_layernorm.weight\"],\n",
        "        f\"model.layers.{l}.input_layernorm.weight\",\n",
        "    )\n",
        "    block.post_attention_layernorm.scale = assign(\n",
        "        block.post_attention_layernorm.scale,\n",
        "        params[f\"model.layers.{l}.post_attention_layernorm.weight\"],\n",
        "        f\"model.layers.{l}.post_attention_layernorm.weight\",\n",
        "    )\n",
        "    # Pre‑ and post‑feed forward norms\n",
        "    pre_key = f\"model.layers.{l}.pre_feedforward_layernorm.weight\"\n",
        "    post_key = f\"model.layers.{l}.post_feedforward_layernorm.weight\"\n",
        "    if pre_key in params:\n",
        "      block.pre_feedforward_layernorm.scale = assign(\n",
        "          block.pre_feedforward_layernorm.scale,\n",
        "          params[pre_key],\n",
        "          pre_key,\n",
        "      )\n",
        "    if post_key in params:\n",
        "      block.post_feedforward_layernorm.scale = assign(\n",
        "          block.post_feedforward_layernorm.scale,\n",
        "          params[post_key],\n",
        "          post_key,\n",
        "      )\n",
        "\n",
        "  # Final LayerNorm\n",
        "  if \"model.norm.weight\" in params:\n",
        "    model.final_norm.scale = assign(\n",
        "        model.final_norm.scale,\n",
        "        params[\"model.norm.weight\"],\n",
        "        \"model.norm.weight\",\n",
        "    )\n",
        "  # Output head\n",
        "  if \"lm_head.weight\" in params:\n",
        "    model.out_head.weight = assign(\n",
        "        model.out_head.weight,\n",
        "        params[\"lm_head.weight\"],\n",
        "        \"lm_head.weight\",\n",
        "    )\n",
        "  elif \"model.embed_tokens.weight\" in params:\n",
        "    # Weight tying: reuse the embedding weights\n",
        "    model.out_head.weight = assign(\n",
        "        model.out_head.weight,\n",
        "        params[\"model.embed_tokens.weight\"],\n",
        "        \"model.embed_tokens.weight\",\n",
        "    )"
      ],
      "metadata": {
        "id": "28BrAib-Y-ND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "id": "1Bcg6b6dZnzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from safetensors.torch import load_file\n",
        "from huggingface_hub import hf_hub_download, snapshot_download\n",
        "\n",
        "CHOOSE_MODEL = \"270m\"\n",
        "\n",
        "if USE_INSTRUCT_MODEL:\n",
        "  repo_id = f\"google/gemma-3-{CHOOSE_MODEL}-it\"\n",
        "else:\n",
        "  repo_id = f\"google/gemma-3-{CHOOSE_MODEL}\"\n",
        "\n",
        "\n",
        "local_dir = Path(repo_id).parts[-1]\n",
        "\n",
        "if CHOOSE_MODEL == \"270m\":\n",
        "  weights_file = hf_hub_download(\n",
        "      repo_id=repo_id,\n",
        "      filename=\"model.safetensors\",\n",
        "      local_dir=local_dir,\n",
        "  )\n",
        "  weights_dict = load_file(weights_file)\n",
        "else:\n",
        "  repo_dir = snapshot_download(repo_id=repo_id, local_dir=local_dir)\n",
        "  index_path = os.path.join(repo_dir, \"model.safetensors.index.json\")\n",
        "  with open(index_path, \"r\") as f:\n",
        "    index = json.load(f)\n",
        "\n",
        "  weights_dict = {}\n",
        "  for filename in set(index[\"weight_map\"].values()):\n",
        "    shard_path = os.path.join(repo_dir, filename)\n",
        "    shard = load_file(shard_path)\n",
        "    weights_dict.update(shard)\n",
        "\n",
        "load_weights_into_gemma(model, GEMMA3_CONFIG_270M, weights_dict)\n",
        "model.to(device)\n",
        "del weights_dict"
      ],
      "metadata": {
        "id": "tU8MyhrNZuPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Load Tokenizer**"
      ],
      "metadata": {
        "id": "vfsIGij091QO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "\n",
        "# Encodes text into a format the model can process, and decodes the model's output back into human-readable text\n",
        "class GemmaTokenizer:\n",
        "  def __init__(self, tokenizer_file_path: str):\n",
        "    tok_file = Path(tokenizer_file_path)\n",
        "    self._tok = Tokenizer.from_file(str(tok_file))\n",
        "    # Attempt to identify EOS and padding tokens\n",
        "    eos_token = \"<end_of_turn>\"\n",
        "    self.pad_token_id = eos_token\n",
        "    self.eos_token_id = eos_token\n",
        "\n",
        "  def encode(self, text: str) -> list[int]:\n",
        "    return self._tok.encode(text).ids\n",
        "\n",
        "  def decode(self, ids: list[int]) -> str:\n",
        "    return self._tok.decode(ids, skip_special_tokens=False)\n",
        "\n",
        "\n",
        "def apply_chat_template(user_text):\n",
        "  return f\"<start_of_turn>user\\n{user_text}<end_of_turn>\\n<start_of_turn>model\\n\""
      ],
      "metadata": {
        "id": "2g8eS9dtaCgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_file_path = os.path.join(local_dir, \"tokenizer.json\")\n",
        "if not os.path.exists(tokenizer_file_path):\n",
        "  try:\n",
        "    tokenizer_file_path = hf_hub_download(repo_id=repo_id, filename=\"tokenizer.json\", local_dir=local_dir)\n",
        "  except Exception as e:\n",
        "    print(f\"Warning: failed to download tokenizer.json: {e}\")\n",
        "    tokenizer_file_path = \"tokenizer.json\"\n",
        "\n",
        "tokenizer = GemmaTokenizer(tokenizer_file_path=tokenizer_file_path)"
      ],
      "metadata": {
        "id": "bBva-6HM969A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = apply_chat_template(\"Give me a short introduction of Steve Jobs.\")\n",
        "\n",
        "input_token_ids = tokenizer.encode(prompt)\n",
        "text = tokenizer.decode(input_token_ids)\n",
        "text"
      ],
      "metadata": {
        "id": "C4j-3jwX9_nk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Text Generation**"
      ],
      "metadata": {
        "id": "iGsaZFnS-NY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_basic_stream(model, token_ids, max_new_tokens, eos_token_id=None):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for _ in range(max_new_tokens):\n",
        "      out = model(token_ids)[:, -1]\n",
        "      next_token = torch.argmax(out, dim=-1, keepdim=True)\n",
        "\n",
        "      if (eos_token_id is not None and torch.all(next_token == eos_token_id)):\n",
        "        break\n",
        "\n",
        "      yield next_token\n",
        "\n",
        "      token_ids = torch.cat([token_ids, next_token], dim=1)"
      ],
      "metadata": {
        "id": "s05Hx8Mn-Ew6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_token_ids_tensor = torch.tensor(input_token_ids, device=device).unsqueeze(0)\n",
        "\n",
        "\n",
        "for token in generate_text_basic_stream(\n",
        "  model=model,\n",
        "  token_ids=input_token_ids_tensor,\n",
        "  max_new_tokens=100000,\n",
        "  eos_token_id=tokenizer.encode(\"<end_of_turn>\")[-1]\n",
        "):\n",
        "  token_id = token.squeeze(0).tolist()\n",
        "  print(\n",
        "    tokenizer.decode(token_id),\n",
        "    end=\"\",\n",
        "    flush=True\n",
        "  )"
      ],
      "metadata": {
        "id": "RIDvnkai-Mye"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}