{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP85poN8EyiTLMlftcvl5gB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dd8d05b942d34485ba1d0cfe5eeb6558": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1d734c3b171146af86dda9df12394d2a",
              "IPY_MODEL_f310e3e6d3f84cc4abd3c5f8fdffe575",
              "IPY_MODEL_51b8b0dabd1847d899f80425b20b3f18",
              "IPY_MODEL_d76e089bca8947b68b8021c1ba6bf9f2",
              "IPY_MODEL_ac892afdfb48473a9c6f8cb3d6ce863b"
            ],
            "layout": "IPY_MODEL_bbc1fd290bce44a391142259f244dd43",
            "tabbable": null,
            "tooltip": null
          }
        },
        "1d734c3b171146af86dda9df12394d2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_002e0fd3b2ba4e94a89a4ae1adbe4ebd",
            "placeholder": "​",
            "style": "IPY_MODEL_3a39e253eee0423f8f8929267117422e",
            "tabbable": null,
            "tooltip": null,
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "f310e3e6d3f84cc4abd3c5f8fdffe575": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_allow_html": false,
            "disabled": false,
            "layout": "IPY_MODEL_f115bc11bdeb42bb91fe32977d8fd877",
            "placeholder": "​",
            "style": "IPY_MODEL_8f158a13af564a6c8852b90ef212f75f",
            "tabbable": null,
            "tooltip": null,
            "value": ""
          }
        },
        "51b8b0dabd1847d899f80425b20b3f18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_allow_html": false,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_020a3da5a07b4947a2c26f0ca0091bec",
            "style": "IPY_MODEL_a1425f13643740fb9fbdbf86fead735b",
            "tabbable": null,
            "tooltip": null,
            "value": true
          }
        },
        "d76e089bca8947b68b8021c1ba6bf9f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_5d05db9defae499abf3615967a010403",
            "style": "IPY_MODEL_d3537a5ccf18409190f431a52286e108",
            "tabbable": null,
            "tooltip": null
          }
        },
        "ac892afdfb48473a9c6f8cb3d6ce863b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_66d5a11b80644887af6e1514f83fdc13",
            "placeholder": "​",
            "style": "IPY_MODEL_d0d538c68b5a436ca0b64d9ccde4744c",
            "tabbable": null,
            "tooltip": null,
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "bbc1fd290bce44a391142259f244dd43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "002e0fd3b2ba4e94a89a4ae1adbe4ebd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a39e253eee0423f8f8929267117422e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "f115bc11bdeb42bb91fe32977d8fd877": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f158a13af564a6c8852b90ef212f75f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "TextStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "020a3da5a07b4947a2c26f0ca0091bec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1425f13643740fb9fbdbf86fead735b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "CheckboxStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": ""
          }
        },
        "5d05db9defae499abf3615967a010403": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3537a5ccf18409190f431a52286e108": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_family": null,
            "font_size": null,
            "font_style": null,
            "font_variant": null,
            "font_weight": null,
            "text_color": null,
            "text_decoration": null
          }
        },
        "66d5a11b80644887af6e1514f83fdc13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0d538c68b5a436ca0b64d9ccde4744c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/p1kalys/Gemma-3-270M/blob/main/gemma_3_270m_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4odORH0nS3xf"
      },
      "outputs": [],
      "source": [
        "!pip install -r https://raw.githubusercontent.com/p1kalys/Gemma-3-270M/refs/heads/main/requirements.txt --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "pkgs = [\n",
        "    \"huggingface_hub\",  # to download pretrained weights\n",
        "    \"tokenizers\",       # to implement the tokenizer\n",
        "    \"torch\",            # to implement the model\n",
        "]\n",
        "for p in pkgs:\n",
        "    print(f\"{p} version: {version(p)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4eoVnO-UBhH",
        "outputId": "8b5bfb75-8f1d-43a1-e6b2-bc7688533eed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "huggingface_hub version: 0.34.4\n",
            "tokenizers version: 0.21.4\n",
            "torch version: 2.8.0+cu126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "USE_INSTRUCT_MODEL = True"
      ],
      "metadata": {
        "id": "i1UEc1iYUNJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Architecture**"
      ],
      "metadata": {
        "id": "CDT-CRntUZAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Applies non linear transformation to the input tensor using two parallel linear layers, GELU (Guassian Error Linear Unit) activation function and a final linear layer to project output back to the original embedding dimension\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
        "    self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
        "    self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x_fc1 = self.fc1(x)\n",
        "    x_fc2 = self.fc2(x)\n",
        "    x = nn.functional.gelu(x_fc1, approximate=\"tanh\") * x_fc2\n",
        "    return self.fc3(x)"
      ],
      "metadata": {
        "id": "KlazpD65URaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizes the output of layers in the Gemma model\n",
        "class RMSNorm(nn.Module):\n",
        "  def __init__(self, emb_dim, eps=1e-6, bias=False):\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    # Gemma3 stores zero-centered weights and uses (1 + weight) during forward\n",
        "    self.scale = nn.Parameter(torch.zeros(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim)) if bias else None\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Match HF Gemma3: compute norm in float32, then scale by (1 + w)\n",
        "    input_dtype = x.dtype\n",
        "    x_f = x.float()\n",
        "    var = x_f.pow(2).mean(dim=-1, keepdim=True)\n",
        "    x_norm = x_f * torch.rsqrt(var + self.eps)\n",
        "    out = x_norm * (1.0 + self.scale.float())\n",
        "\n",
        "    if self.shift is not None:\n",
        "      out = out + self.shift.float()\n",
        "\n",
        "    return out.to(input_dtype)"
      ],
      "metadata": {
        "id": "lxo6hUMZUT0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Rotary Position Embeddings (RoPE) - injects positional information into the attention mechanism of a transformer model\n",
        "def compute_rope_params(head_dim, theta_base=10_000, context_length=4096, dtype=torch.float32):\n",
        "  assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
        "\n",
        "  # Compute the inverse frequencies\n",
        "  inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype=dtype)[: (head_dim // 2)].float() / head_dim))\n",
        "\n",
        "  # Generate position indices\n",
        "  positions = torch.arange(context_length, dtype=dtype)\n",
        "\n",
        "  # Compute the angles\n",
        "  angles = positions[:, None] * inv_freq[None, :]  # Shape: (context_length, head_dim // 2)\n",
        "\n",
        "  # Expand angles to match the head_dim\n",
        "  angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n",
        "\n",
        "  # Precompute sine and cosine\n",
        "  cos = torch.cos(angles)\n",
        "  sin = torch.sin(angles)\n",
        "\n",
        "  return cos, sin"
      ],
      "metadata": {
        "id": "-nFKsC4OVds6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Applying RoPE to input tensor `x`\n",
        "def apply_rope(x, cos, sin):\n",
        "  # x: (batch_size, num_heads, seq_len, head_dim)\n",
        "  batch_size, num_heads, seq_len, head_dim = x.shape\n",
        "  assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
        "\n",
        "  # Split x into first half and second half\n",
        "  x1 = x[..., : head_dim // 2]  # First half\n",
        "  x2 = x[..., head_dim // 2 :]  # Second half\n",
        "\n",
        "  # Adjust sin and cos shapes\n",
        "  cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim)\n",
        "  sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "  # Apply the rotary transformation\n",
        "  rotated = torch.cat((-x2, x1), dim=-1)\n",
        "  x_rotated = (x * cos) + (rotated * sin)\n",
        "\n",
        "  # It's ok to use lower-precision after applying cos and sin rotation\n",
        "  return x_rotated.to(dtype=x.dtype)"
      ],
      "metadata": {
        "id": "d7_dDmstVnYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GQA - an optimization of standard multi-head attention which is the grouping of key and value heads to reduce computation.\n",
        "class GroupedQueryAttention(nn.Module):\n",
        "  def __init__(self, d_in, num_heads, num_kv_groups, head_dim=None, qk_norm=False, query_pre_attn_scalar=None, dtype=None):\n",
        "    super().__init__()\n",
        "    assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
        "\n",
        "    self.num_heads = num_heads\n",
        "    self.num_kv_groups = num_kv_groups\n",
        "    self.group_size = num_heads // num_kv_groups\n",
        "\n",
        "    if head_dim is None:\n",
        "      assert d_in % num_heads == 0, \"`d_in` must be divisible by `num_heads` if `head_dim` is not set\"\n",
        "      head_dim = d_in // num_heads\n",
        "\n",
        "    self.head_dim = head_dim\n",
        "    self.d_out = num_heads * head_dim\n",
        "\n",
        "    self.W_query = nn.Linear(d_in, self.d_out, bias=False, dtype=dtype)\n",
        "    self.W_key = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
        "    self.W_value = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype)\n",
        "\n",
        "    self.out_proj = nn.Linear(self.d_out, d_in, bias=False, dtype=dtype)\n",
        "\n",
        "    if qk_norm:\n",
        "      self.q_norm = RMSNorm(head_dim, eps=1e-6)\n",
        "      self.k_norm = RMSNorm(head_dim, eps=1e-6)\n",
        "    else:\n",
        "      self.q_norm = self.k_norm = None\n",
        "\n",
        "    if query_pre_attn_scalar is not None:\n",
        "      self.scaling = (query_pre_attn_scalar) ** -0.5\n",
        "    else:\n",
        "      self.scaling = (head_dim) ** -0.5\n",
        "\n",
        "\n",
        "  def forward(self, x, mask, cos, sin):\n",
        "    b, num_tokens, _ = x.shape\n",
        "\n",
        "    # Apply projections\n",
        "    queries = self.W_query(x)  # (b, num_tokens, num_heads * head_dim)\n",
        "    keys = self.W_key(x)       # (b, num_tokens, num_kv_groups * head_dim)\n",
        "    values = self.W_value(x)   # (b, num_tokens, num_kv_groups * head_dim)\n",
        "\n",
        "    # Reshape\n",
        "    queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "    keys = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
        "    values = values.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
        "\n",
        "    # Optional normalization\n",
        "    if self.q_norm:\n",
        "      queries = self.q_norm(queries)\n",
        "    if self.k_norm:\n",
        "      keys = self.k_norm(keys)\n",
        "\n",
        "    # Apply RoPE\n",
        "    queries = apply_rope(queries, cos, sin)\n",
        "    keys = apply_rope(keys, cos, sin)\n",
        "\n",
        "    # Expand K and V to match number of heads\n",
        "    keys = keys.repeat_interleave(self.group_size, dim=1)\n",
        "    values = values.repeat_interleave(self.group_size, dim=1)\n",
        "\n",
        "    # Scale queries\n",
        "    queries = queries * self.scaling\n",
        "\n",
        "    # Attention\n",
        "    attn_scores = queries @ keys.transpose(2, 3)\n",
        "    attn_scores = attn_scores.masked_fill(mask, -torch.inf)\n",
        "    attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "    context = (attn_weights @ values).transpose(1, 2).reshape(b, num_tokens, self.d_out)\n",
        "    return self.out_proj(context)"
      ],
      "metadata": {
        "id": "H5PPq--PWdVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TransformerBlock processes the input through a sequence of normalization, attention, residual connection, normalization, feed-forward, and another residual connection.\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, cfg: dict, attn_type: str):\n",
        "    super().__init__()\n",
        "    self.attn_type = attn_type\n",
        "    self.att = GroupedQueryAttention(\n",
        "        d_in=cfg[\"emb_dim\"],\n",
        "        num_heads=cfg[\"n_heads\"],\n",
        "        num_kv_groups=cfg[\"n_kv_groups\"],\n",
        "        head_dim=cfg[\"head_dim\"],\n",
        "        qk_norm=cfg[\"qk_norm\"],\n",
        "        query_pre_attn_scalar=cfg[\"query_pre_attn_scalar\"],\n",
        "        dtype=cfg[\"dtype\"])\n",
        "    self.ff = FeedForward(cfg)\n",
        "    self.input_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
        "    self.post_attention_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
        "    self.pre_feedforward_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
        "    self.post_feedforward_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
        "\n",
        "  def forward(self, x, mask_global, mask_local, cos_global, sin_global, cos_local, sin_local):\n",
        "    # Shortcut connection for attention block\n",
        "    shortcut = x\n",
        "    x = self.input_layernorm(x)\n",
        "\n",
        "    if self.attn_type == \"sliding_attention\":\n",
        "      attn_mask = mask_local\n",
        "      cos = cos_local\n",
        "      sin = sin_local\n",
        "    else:\n",
        "      attn_mask = mask_global\n",
        "      cos = cos_global\n",
        "      sin = sin_global\n",
        "\n",
        "    x_attn = self.att(x, attn_mask, cos, sin)\n",
        "    x_attn = self.post_attention_layernorm(x_attn)\n",
        "    x = shortcut + x_attn\n",
        "\n",
        "    # Shortcut connection for feed forward block\n",
        "    shortcut = x\n",
        "    x_ffn = self.pre_feedforward_layernorm(x)\n",
        "    x_ffn = self.ff(x_ffn)\n",
        "    x_ffn = self.post_feedforward_layernorm(x_ffn)\n",
        "    x = shortcut + x_ffn\n",
        "    return x"
      ],
      "metadata": {
        "id": "QXEwkKZTW-dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Orchestrates the flow of data through the entire network, applying token embeddings, multiple transformer blocks with different attention mechanisms and masking, final normalization, and an output head to produce the final token logits.\n",
        "class Gemma3Model(nn.Module):\n",
        "  def __init__(self, cfg):\n",
        "    super().__init__()\n",
        "    assert cfg[\"layer_types\"] is not None and len(cfg[\"layer_types\"]) == cfg[\"n_layers\"]\n",
        "\n",
        "    # Main model parameters\n",
        "    self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
        "\n",
        "    self.blocks = nn.ModuleList([\n",
        "        TransformerBlock(cfg, attn_type)for attn_type in cfg[\"layer_types\"]\n",
        "    ])\n",
        "\n",
        "    self.final_norm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6)\n",
        "    self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
        "    self.cfg = cfg\n",
        "\n",
        "    # Reusuable utilities\n",
        "    cos_local, sin_local = compute_rope_params(\n",
        "        head_dim=cfg[\"head_dim\"],\n",
        "        theta_base=cfg[\"rope_local_base\"],\n",
        "        context_length=cfg[\"context_length\"],\n",
        "        dtype=torch.float32,\n",
        "    )\n",
        "    cos_global, sin_global = compute_rope_params(\n",
        "        head_dim=cfg[\"head_dim\"],\n",
        "        theta_base=cfg[\"rope_base\"],\n",
        "        context_length=cfg[\"context_length\"],\n",
        "        dtype=torch.float32,\n",
        "    )\n",
        "    self.register_buffer(\"cos_local\", cos_local, persistent=False)\n",
        "    self.register_buffer(\"sin_local\", sin_local, persistent=False)\n",
        "    self.register_buffer(\"cos_global\", cos_global, persistent=False)\n",
        "    self.register_buffer(\"sin_global\", sin_global, persistent=False)\n",
        "\n",
        "  def _create_masks(self, seq_len, device):\n",
        "    ones = torch.ones((seq_len, seq_len), dtype=torch.bool, device=device)\n",
        "\n",
        "    # mask_global (future is masked: j > i)\n",
        "    #     j:  0 1 2 3 4 5 6 7\n",
        "    #  i\n",
        "    #     0:  0 1 1 1 1 1 1 1\n",
        "    #     1:  0 0 1 1 1 1 1 1\n",
        "    #     2:  0 0 0 1 1 1 1 1\n",
        "    #     3:  0 0 0 0 1 1 1 1\n",
        "    #     4:  0 0 0 0 0 1 1 1\n",
        "    #     5:  0 0 0 0 0 0 1 1\n",
        "    #     6:  0 0 0 0 0 0 0 1\n",
        "    #     7:  0 0 0 0 0 0 0 0\n",
        "    mask_global = torch.triu(ones, diagonal=1)\n",
        "\n",
        "    # far_past (too far back is masked: i - j >= sliding_window)\n",
        "    # where sliding_window = 4\n",
        "    #     j:  0 1 2 3 4 5 6 7\n",
        "    #  i\n",
        "    #     0:  0 0 0 0 0 0 0 0\n",
        "    #     1:  0 0 0 0 0 0 0 0\n",
        "    #     2:  0 0 0 0 0 0 0 0\n",
        "    #     3:  0 0 0 0 0 0 0 0\n",
        "    #     4:  1 0 0 0 0 0 0 0\n",
        "    #     5:  1 1 0 0 0 0 0 0\n",
        "    #     6:  1 1 1 0 0 0 0 0\n",
        "    #     7:  1 1 1 1 0 0 0 0\n",
        "    far_past = torch.triu(ones, diagonal=self.cfg[\"sliding_window\"]).T\n",
        "\n",
        "    # Local (sliding_window) = future OR far-past\n",
        "    # mask_local\n",
        "    #     j:  0 1 2 3 4 5 6 7\n",
        "    # i\n",
        "    # 0:      0 1 1 1 1 1 1 1\n",
        "    # 1:      0 0 1 1 1 1 1 1\n",
        "    # 2:      0 0 0 1 1 1 1 1\n",
        "    # 3:      0 0 0 0 1 1 1 1\n",
        "    # 4:      1 0 0 0 0 1 1 1\n",
        "    # 5:      1 1 0 0 0 0 1 1\n",
        "    # 6:      1 1 1 0 0 0 0 1\n",
        "    # 7:      1 1 1 1 0 0 0 0\n",
        "    mask_local = mask_global | far_past\n",
        "    return mask_global, mask_local\n",
        "\n",
        "  def forward(self, input_ids):\n",
        "    # Forward pass\n",
        "    b, seq_len = input_ids.shape\n",
        "    x = self.tok_emb(input_ids) * (self.cfg[\"emb_dim\"] ** 0.5)\n",
        "    mask_global, mask_local = self._create_masks(seq_len, x.device)\n",
        "\n",
        "    for block in self.blocks:\n",
        "      x = block(\n",
        "          x,\n",
        "          mask_global=mask_global,\n",
        "          mask_local=mask_local,\n",
        "          cos_global=self.cos_global,\n",
        "          sin_global=self.sin_global,\n",
        "          cos_local=self.cos_local,\n",
        "          sin_local=self.sin_local,\n",
        "      )\n",
        "\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x.to(self.cfg[\"dtype\"]))\n",
        "    return logits"
      ],
      "metadata": {
        "id": "BxW9Stc2XW9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Initialize model**"
      ],
      "metadata": {
        "id": "Y9xZqsSsXtSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GEMMA3_CONFIG_270M = {\n",
        "    \"vocab_size\": 262_144,\n",
        "    \"context_length\": 32_768,\n",
        "    \"emb_dim\": 640,\n",
        "    \"n_heads\": 4,\n",
        "    \"n_layers\": 18,\n",
        "    \"hidden_dim\": 2048,\n",
        "    \"head_dim\": 256,\n",
        "    \"qk_norm\": True,\n",
        "    \"n_kv_groups\": 1,\n",
        "    \"rope_local_base\": 10_000.0,\n",
        "    \"rope_base\": 1_000_000.0,\n",
        "    \"sliding_window\": 512,\n",
        "      \"layer_types\": [\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"full_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"full_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"full_attention\"\n",
        "    ],\n",
        "    \"dtype\": torch.bfloat16,\n",
        "    \"query_pre_attn_scalar\": 256,\n",
        "}"
      ],
      "metadata": {
        "id": "_GOalGWBXqDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = Gemma3Model(GEMMA3_CONFIG_270M)"
      ],
      "metadata": {
        "id": "8QwN950CXx-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XN0SLEWGX0Ye",
        "outputId": "b0dac6f8-66fa-4669-8e11-61d8b84c65b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Gemma3Model(\n",
              "  (tok_emb): Embedding(262144, 640)\n",
              "  (blocks): ModuleList(\n",
              "    (0-17): 18 x TransformerBlock(\n",
              "      (att): GroupedQueryAttention(\n",
              "        (W_query): Linear(in_features=640, out_features=1024, bias=False)\n",
              "        (W_key): Linear(in_features=640, out_features=256, bias=False)\n",
              "        (W_value): Linear(in_features=640, out_features=256, bias=False)\n",
              "        (out_proj): Linear(in_features=1024, out_features=640, bias=False)\n",
              "        (q_norm): RMSNorm()\n",
              "        (k_norm): RMSNorm()\n",
              "      )\n",
              "      (ff): FeedForward(\n",
              "        (fc1): Linear(in_features=640, out_features=2048, bias=False)\n",
              "        (fc2): Linear(in_features=640, out_features=2048, bias=False)\n",
              "        (fc3): Linear(in_features=2048, out_features=640, bias=False)\n",
              "      )\n",
              "      (input_layernorm): RMSNorm()\n",
              "      (post_attention_layernorm): RMSNorm()\n",
              "      (pre_feedforward_layernorm): RMSNorm()\n",
              "      (post_feedforward_layernorm): RMSNorm()\n",
              "    )\n",
              "  )\n",
              "  (final_norm): RMSNorm()\n",
              "  (out_head): Linear(in_features=640, out_features=262144, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model(torch.tensor([1, 2, 3]).unsqueeze(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gH6od1u9Ysy-",
        "outputId": "cc55d8e1-a0e6-4139-e4b9-1237b2a2aa20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.7539,  0.1060,  0.4805,  ...,  0.9375,  0.4043, -0.2383],\n",
              "         [-0.3418, -0.0576,  0.8984,  ..., -0.2432,  0.4629,  0.8242],\n",
              "         [-0.2695, -0.3281,  0.4102,  ...,  0.8750, -0.9727,  0.9844]]],\n",
              "       dtype=torch.bfloat16, grad_fn=<UnsafeViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params:,}\")\n",
        "\n",
        "# Account for weight tying\n",
        "total_params_normalized = total_params - model.tok_emb.weight.numel()\n",
        "print(f\"\\nTotal number of unique parameters: {total_params_normalized:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swD9yD2FYwHD",
        "outputId": "d13c3c1b-80d4-49f7-8ad5-5b64c0cc2d57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 435,870,336\n",
            "\n",
            "Total number of unique parameters: 268,098,176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model_memory_size(model, input_dtype=torch.float32):\n",
        "  total_params = 0\n",
        "  total_grads = 0\n",
        "  for param in model.parameters():\n",
        "    # Calculate total number of elements per parameter\n",
        "    param_size = param.numel()\n",
        "    total_params += param_size\n",
        "    # Check if gradients are stored for this parameter\n",
        "    if param.requires_grad:\n",
        "      total_grads += param_size\n",
        "\n",
        "  # Calculate buffer size (non-parameters that require memory)\n",
        "  total_buffers = sum(buf.numel() for buf in model.buffers())\n",
        "\n",
        "  # Size in bytes = (Number of elements) * (Size of each element in bytes)\n",
        "  # We assume parameters and gradients are stored in the same type as input dtype\n",
        "  element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
        "  total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
        "\n",
        "  # Convert bytes to gigabytes\n",
        "  total_memory_gb = total_memory_bytes / (1024**3)\n",
        "\n",
        "  return total_memory_gb\n",
        "\n",
        "print(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.2f} GB\")\n",
        "print(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CE0I6eOWYysP",
        "outputId": "a7f832ae-179b-457b-d896-57445ea232c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "float32 (PyTorch default): 3.37 GB\n",
            "bfloat16: 1.69 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "model.to(device);"
      ],
      "metadata": {
        "id": "uXz0OJIBY7At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Load pretrained weights**"
      ],
      "metadata": {
        "id": "uZTN3iXJZIQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transfers the learned knowledge from a pretrained model to custom model implementation, allowing us to leverage the benefits of pretraining without training from scratch.\n",
        "def load_weights_into_gemma(Gemma3Model, param_config, params):\n",
        "  def assign(left, right, tensor_name=\"unknown\"):\n",
        "    if left.shape != right.shape:\n",
        "      raise ValueError(\n",
        "          f\"Shape mismatch in tensor '{tensor_name}'. Left: {left.shape}, Right: {right.shape}\"\n",
        "      )\n",
        "    return torch.nn.Parameter(right.clone().detach() if isinstance(right, torch.Tensor) else torch.tensor(right))\n",
        "\n",
        "  # Embedding weights\n",
        "  if \"model.embed_tokens.weight\" in params:\n",
        "    model.tok_emb.weight = assign(\n",
        "        model.tok_emb.weight,\n",
        "        params[\"model.embed_tokens.weight\"],\n",
        "        \"model.embed_tokens.weight\",\n",
        "    )\n",
        "\n",
        "  # Iterate over transformer layers\n",
        "  for l in range(param_config[\"n_layers\"]):\n",
        "    block = model.blocks[l]\n",
        "    att = block.att\n",
        "    # Attention projections\n",
        "    att.W_query.weight = assign(\n",
        "        att.W_query.weight,\n",
        "        params[f\"model.layers.{l}.self_attn.q_proj.weight\"],\n",
        "        f\"model.layers.{l}.self_attn.q_proj.weight\",\n",
        "    )\n",
        "    att.W_key.weight = assign(\n",
        "        att.W_key.weight,\n",
        "        params[f\"model.layers.{l}.self_attn.k_proj.weight\"],\n",
        "        f\"model.layers.{l}.self_attn.k_proj.weight\",\n",
        "    )\n",
        "    att.W_value.weight = assign(\n",
        "        att.W_value.weight,\n",
        "        params[f\"model.layers.{l}.self_attn.v_proj.weight\"],\n",
        "        f\"model.layers.{l}.self_attn.v_proj.weight\",\n",
        "    )\n",
        "    att.out_proj.weight = assign(\n",
        "        att.out_proj.weight,\n",
        "        params[f\"model.layers.{l}.self_attn.o_proj.weight\"],\n",
        "        f\"model.layers.{l}.self_attn.o_proj.weight\",\n",
        "    )\n",
        "    # QK normalization weights\n",
        "    att.q_norm.scale = assign(\n",
        "        att.q_norm.scale,\n",
        "        params[f\"model.layers.{l}.self_attn.q_norm.weight\"],\n",
        "        f\"model.layers.{l}.self_attn.q_norm.weight\",\n",
        "    )\n",
        "    att.k_norm.scale = assign(\n",
        "        att.k_norm.scale,\n",
        "        params[f\"model.layers.{l}.self_attn.k_norm.weight\"],\n",
        "        f\"model.layers.{l}.self_attn.k_norm.weight\",\n",
        "    )\n",
        "    # Feed forward weights\n",
        "    block.ff.fc1.weight = assign(\n",
        "        block.ff.fc1.weight,\n",
        "        params[f\"model.layers.{l}.mlp.gate_proj.weight\"],\n",
        "        f\"model.layers.{l}.mlp.gate_proj.weight\",\n",
        "    )\n",
        "    block.ff.fc2.weight = assign(\n",
        "        block.ff.fc2.weight,\n",
        "        params[f\"model.layers.{l}.mlp.up_proj.weight\"],\n",
        "        f\"model.layers.{l}.mlp.up_proj.weight\",\n",
        "    )\n",
        "    block.ff.fc3.weight = assign(\n",
        "        block.ff.fc3.weight,\n",
        "        params[f\"model.layers.{l}.mlp.down_proj.weight\"],\n",
        "        f\"model.layers.{l}.mlp.down_proj.weight\",\n",
        "    )\n",
        "    # LayerNorm weights\n",
        "    block.input_layernorm.scale = assign(\n",
        "        block.input_layernorm.scale,\n",
        "        params[f\"model.layers.{l}.input_layernorm.weight\"],\n",
        "        f\"model.layers.{l}.input_layernorm.weight\",\n",
        "    )\n",
        "    block.post_attention_layernorm.scale = assign(\n",
        "        block.post_attention_layernorm.scale,\n",
        "        params[f\"model.layers.{l}.post_attention_layernorm.weight\"],\n",
        "        f\"model.layers.{l}.post_attention_layernorm.weight\",\n",
        "    )\n",
        "    # Pre‑ and post‑feed forward norms\n",
        "    pre_key = f\"model.layers.{l}.pre_feedforward_layernorm.weight\"\n",
        "    post_key = f\"model.layers.{l}.post_feedforward_layernorm.weight\"\n",
        "    if pre_key in params:\n",
        "      block.pre_feedforward_layernorm.scale = assign(\n",
        "          block.pre_feedforward_layernorm.scale,\n",
        "          params[pre_key],\n",
        "          pre_key,\n",
        "      )\n",
        "    if post_key in params:\n",
        "      block.post_feedforward_layernorm.scale = assign(\n",
        "          block.post_feedforward_layernorm.scale,\n",
        "          params[post_key],\n",
        "          post_key,\n",
        "      )\n",
        "\n",
        "  # Final LayerNorm\n",
        "  if \"model.norm.weight\" in params:\n",
        "    model.final_norm.scale = assign(\n",
        "        model.final_norm.scale,\n",
        "        params[\"model.norm.weight\"],\n",
        "        \"model.norm.weight\",\n",
        "    )\n",
        "  # Output head\n",
        "  if \"lm_head.weight\" in params:\n",
        "    model.out_head.weight = assign(\n",
        "        model.out_head.weight,\n",
        "        params[\"lm_head.weight\"],\n",
        "        \"lm_head.weight\",\n",
        "    )\n",
        "  elif \"model.embed_tokens.weight\" in params:\n",
        "    # Weight tying: reuse the embedding weights\n",
        "    model.out_head.weight = assign(\n",
        "        model.out_head.weight,\n",
        "        params[\"model.embed_tokens.weight\"],\n",
        "        \"model.embed_tokens.weight\",\n",
        "    )"
      ],
      "metadata": {
        "id": "28BrAib-Y-ND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "dd8d05b942d34485ba1d0cfe5eeb6558",
            "1d734c3b171146af86dda9df12394d2a",
            "f310e3e6d3f84cc4abd3c5f8fdffe575",
            "51b8b0dabd1847d899f80425b20b3f18",
            "d76e089bca8947b68b8021c1ba6bf9f2",
            "ac892afdfb48473a9c6f8cb3d6ce863b",
            "bbc1fd290bce44a391142259f244dd43",
            "002e0fd3b2ba4e94a89a4ae1adbe4ebd",
            "3a39e253eee0423f8f8929267117422e",
            "f115bc11bdeb42bb91fe32977d8fd877",
            "8f158a13af564a6c8852b90ef212f75f",
            "020a3da5a07b4947a2c26f0ca0091bec",
            "a1425f13643740fb9fbdbf86fead735b",
            "5d05db9defae499abf3615967a010403",
            "d3537a5ccf18409190f431a52286e108",
            "66d5a11b80644887af6e1514f83fdc13",
            "d0d538c68b5a436ca0b64d9ccde4744c"
          ]
        },
        "id": "1Bcg6b6dZnzz",
        "outputId": "7cd55869-1fa7-447f-e4aa-8828982f1585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd8d05b942d34485ba1d0cfe5eeb6558"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from safetensors.torch import load_file\n",
        "from huggingface_hub import hf_hub_download, snapshot_download\n",
        "\n",
        "CHOOSE_MODEL = \"270m\"\n",
        "\n",
        "if USE_INSTRUCT_MODEL:\n",
        "  repo_id = f\"google/gemma-3-{CHOOSE_MODEL}-it\"\n",
        "else:\n",
        "  repo_id = f\"google/gemma-3-{CHOOSE_MODEL}\"\n",
        "\n",
        "\n",
        "local_dir = Path(repo_id).parts[-1]\n",
        "\n",
        "if CHOOSE_MODEL == \"270m\":\n",
        "  weights_file = hf_hub_download(\n",
        "      repo_id=repo_id,\n",
        "      filename=\"model.safetensors\",\n",
        "      local_dir=local_dir,\n",
        "  )\n",
        "  weights_dict = load_file(weights_file)\n",
        "else:\n",
        "  repo_dir = snapshot_download(repo_id=repo_id, local_dir=local_dir)\n",
        "  index_path = os.path.join(repo_dir, \"model.safetensors.index.json\")\n",
        "  with open(index_path, \"r\") as f:\n",
        "    index = json.load(f)\n",
        "\n",
        "  weights_dict = {}\n",
        "  for filename in set(index[\"weight_map\"].values()):\n",
        "    shard_path = os.path.join(repo_dir, filename)\n",
        "    shard = load_file(shard_path)\n",
        "    weights_dict.update(shard)\n",
        "\n",
        "load_weights_into_gemma(model, GEMMA3_CONFIG_270M, weights_dict)\n",
        "model.to(device)\n",
        "del weights_dict"
      ],
      "metadata": {
        "id": "tU8MyhrNZuPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Load Tokenizer**"
      ],
      "metadata": {
        "id": "vfsIGij091QO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "\n",
        "# Encodes text into a format the model can process, and decodes the model's output back into human-readable text\n",
        "class GemmaTokenizer:\n",
        "  def __init__(self, tokenizer_file_path: str):\n",
        "    tok_file = Path(tokenizer_file_path)\n",
        "    self._tok = Tokenizer.from_file(str(tok_file))\n",
        "    # Attempt to identify EOS and padding tokens\n",
        "    eos_token = \"<end_of_turn>\"\n",
        "    self.pad_token_id = eos_token\n",
        "    self.eos_token_id = eos_token\n",
        "\n",
        "  def encode(self, text: str) -> list[int]:\n",
        "    return self._tok.encode(text).ids\n",
        "\n",
        "  def decode(self, ids: list[int]) -> str:\n",
        "    return self._tok.decode(ids, skip_special_tokens=False)\n",
        "\n",
        "\n",
        "def apply_chat_template(user_text):\n",
        "  return f\"<start_of_turn>user\\n{user_text}<end_of_turn>\\n<start_of_turn>model\\n\""
      ],
      "metadata": {
        "id": "2g8eS9dtaCgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_file_path = os.path.join(local_dir, \"tokenizer.json\")\n",
        "if not os.path.exists(tokenizer_file_path):\n",
        "  try:\n",
        "    tokenizer_file_path = hf_hub_download(repo_id=repo_id, filename=\"tokenizer.json\", local_dir=local_dir)\n",
        "  except Exception as e:\n",
        "    print(f\"Warning: failed to download tokenizer.json: {e}\")\n",
        "    tokenizer_file_path = \"tokenizer.json\"\n",
        "\n",
        "tokenizer = GemmaTokenizer(tokenizer_file_path=tokenizer_file_path)"
      ],
      "metadata": {
        "id": "bBva-6HM969A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = apply_chat_template(\"Give me a short introduction of Steve Jobs.\")\n",
        "\n",
        "input_token_ids = tokenizer.encode(prompt)\n",
        "text = tokenizer.decode(input_token_ids)\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "C4j-3jwX9_nk",
        "outputId": "e4f87d03-a5b1-4b62-dfc0-126f19e71c6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<bos><start_of_turn>user\\nGive me a short introduction of Steve Jobs.<end_of_turn>\\n<start_of_turn>model\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Text Generation**"
      ],
      "metadata": {
        "id": "iGsaZFnS-NY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_basic_stream(model, token_ids, max_new_tokens, eos_token_id=None):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for _ in range(max_new_tokens):\n",
        "      out = model(token_ids)[:, -1]\n",
        "      next_token = torch.argmax(out, dim=-1, keepdim=True)\n",
        "\n",
        "      if (eos_token_id is not None and torch.all(next_token == eos_token_id)):\n",
        "        break\n",
        "\n",
        "      yield next_token\n",
        "\n",
        "      token_ids = torch.cat([token_ids, next_token], dim=1)"
      ],
      "metadata": {
        "id": "s05Hx8Mn-Ew6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_token_ids_tensor = torch.tensor(input_token_ids, device=device).unsqueeze(0)\n",
        "\n",
        "\n",
        "for token in generate_text_basic_stream(\n",
        "  model=model,\n",
        "  token_ids=input_token_ids_tensor,\n",
        "  max_new_tokens=100000,\n",
        "  eos_token_id=tokenizer.encode(\"<end_of_turn>\")[-1]\n",
        "):\n",
        "  token_id = token.squeeze(0).tolist()\n",
        "  print(\n",
        "    tokenizer.decode(token_id),\n",
        "    end=\"\",\n",
        "    flush=True\n",
        "  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RIDvnkai-Mye",
        "outputId": "d089f149-9eab-464a-eb69-8c10bb362ba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Steve Jobs was a legendary American entrepreneur and visionary who revolutionized the world of technology and design. He was known for his innovative approach to product development, his focus on user experience, and his commitment to creating a seamless and intuitive digital experience. He is considered one of the most influential figures in the history of technology and has had a profound impact on the world.\n"
          ]
        }
      ]
    }
  ]
}